# Generated by Django 5.1.4 on 2025-01-23 16:09

from django.db import migrations
from django.utils import timezone

def create_initial_configs(apps, schema_editor):
    CrawlerConfig = apps.get_model('crawler', 'CrawlerConfig')
    
    # RSS爬虫配置
    CrawlerConfig.objects.create(
        name='36氪RSS',
        description='抓取36氪的RSS源',
        source_url='https://www.36kr.com/feed',
        crawler_type=1,  # RSS类型
        config_data={
            'item_selector': 'item',
            'title_selector': 'title',
            'description_selector': 'description',
            'link_selector': 'link',
            'pubDate_selector': 'pubDate'
        },
        headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        },
        interval=30,  # 30分钟抓取一次
        status=1,  # 启用状态
        is_active=True,
        created_at=timezone.now(),
        updated_at=timezone.now()
    )
    
    # API爬虫配置
    CrawlerConfig.objects.create(
        name='新闻API',
        description='从新闻API获取最新新闻',
        source_url='https://api.example.com/news',
        crawler_type=2,  # API类型
        config_data={
            'method': 'GET',
            'params': {
                'category': 'technology',
                'limit': 100
            },
            'response_type': 'json',
            'items_path': 'data.articles'
        },
        headers={
            'User-Agent': 'NewsBot/1.0',
            'Accept': 'application/json'
        },
        interval=60,  # 60分钟抓取一次
        status=1,  # 启用状态
        is_active=True,
        created_at=timezone.now(),
        updated_at=timezone.now()
    )
    
    # 网页爬虫配置
    CrawlerConfig.objects.create(
        name='科技新闻网页',
        description='抓取科技新闻网页内容',
        source_url='https://example.com/tech-news',
        crawler_type=3,  # 网页类型
        config_data={
            'list_selector': '.news-list .item',
            'title_selector': '.title a',
            'link_selector': '.title a::attr(href)',
            'description_selector': '.summary',
            'content_selector': '.article-content',
            'date_selector': '.publish-date'
        },
        headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        },
        interval=120,  # 120分钟抓取一次
        status=1,  # 启用状态
        is_active=True,
        created_at=timezone.now(),
        updated_at=timezone.now()
    )

def delete_initial_configs(apps, schema_editor):
    CrawlerConfig = apps.get_model('crawler', 'CrawlerConfig')
    CrawlerConfig.objects.all().delete()

class Migration(migrations.Migration):

    dependencies = [
        ('crawler', '0003_crawlerconfig_is_active_crawlertask_is_test_and_more'),
    ]

    operations = [
        migrations.RunPython(create_initial_configs, delete_initial_configs),
    ]
